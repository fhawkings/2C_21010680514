{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0108e27e",
   "metadata": {},
   "source": [
    "Here is the link to the Github Repository: https://github.com/fhawkings/QT2005_Assessment-3_21010680514 https://github.com/fhawkings/2C_21010680514"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f42ede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all of the relevant libraries.\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "!pip install GeoText\n",
    "from geotext import GeoText\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a044bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the Client ID and the Secret Key from the Reddit API dashboard.\n",
    "\n",
    "CLIENT_ID = 'woKdnKEcjkuZRQGeYo7d4w'\n",
    "SECRET_KEY = 's9a8JGK-pydiDp69EGqG3HgjpBxGjw'\n",
    "\n",
    "# Requesting the authorisation.\n",
    "\n",
    "auth = requests.auth.HTTPBasicAuth(CLIENT_ID, SECRET_KEY)\n",
    "\n",
    "# Creating a dictionary which is containing the necessary data and password access for my token request.\n",
    "\n",
    "data = {\n",
    "    'grant_type': 'password',\n",
    "    'username': 'Felix_Hawkings',\n",
    "    'password': 'kyCtof-9josco-rozsog'\n",
    "}\n",
    "\n",
    "# Identifying version of the API.\n",
    "\n",
    "headers = {'User-Agent': 'MyAPI/0.0.1'}\n",
    "\n",
    "# Send a POST request to obtain the access to the API using login data, auth and headers to give the access token.\n",
    "\n",
    "res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                    auth=auth, data=data, headers=headers)\n",
    "\n",
    "# Extract the access token from the response JSON and store it in a variable.\n",
    "\n",
    "TOKEN = res.json()['access_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e0b0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the access token into the headers to allow access while applying analysis.\n",
    "\n",
    "headers['Authorization'] = f'bearer {TOKEN}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0934f390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sending a GET request to the specified URL with headers to provide access, then retrieving the response and parse it as JSON.\n",
    "\n",
    "requests.get('https://oauth.reddit.com/api/v1/me', headers=headers).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9990bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sending a GET request to test the specified URL with the subreddit of ubrban planning with headers to provide access, then retrieving the response and parse it as JSON.\n",
    "\n",
    "res = requests.get('https://oauth.reddit.com/r/urbanplanning/hot',\n",
    "                   headers=headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2111fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the subreddit names and saving them to a variable.\n",
    "\n",
    "subreddits = ['urbanplanning', 'PlaceMaking', 'Urbanism','parks', 'coffeeshops',\n",
    "              'libraries', 'communitygardening', 'hiking', 'CulturalHeritage', 'Anthropology',\n",
    "              'Neighborhoods', 'PublicSpaces']\n",
    "\n",
    "# Initializing and creating an empty DataFrame and saving it to df.\n",
    "\n",
    "df = pd.DataFrame(columns=['subreddit', 'title', 'selftext'])\n",
    "\n",
    "# Making a GET requests for each subreddit under the variable above and extracting the data.\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    url = f'https://oauth.reddit.com/r/{subreddit}/hot'\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {TOKEN}',\n",
    "        'User-Agent': 'MyAPI/0.0.1'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "# Processing the response and pulling out the children variable which is a list that contains all the data that I need to extract.\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        children = data['data']['children']\n",
    "        \n",
    "# Extracting the data that is relevent and appending it to the DataFrame as well as adding in a failsafe if this fails then it will print a specific line which helps with troubleshooting.\n",
    "\n",
    "        for child in children:\n",
    "            title = child['data']['title']\n",
    "            selftext = child['data']['selftext']\n",
    "            df = df.append({'subreddit': subreddit, 'title': title, 'selftext': selftext}, ignore_index=True)\n",
    "    else:\n",
    "        print(f\"Error retrieving data for subreddit '{subreddit}':\", response.status_code)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626f7eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading NLTK resources that are required to do the analysis in this code block.\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initializing both lemmatizer and stopwords.\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function below to clean and lemmatize all the text.\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\d+', '', text)  # Removing all the numbers.\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Removing all the symbols.\n",
    "\n",
    "# Lemmatize, remove stopwords, and tokenize all the words in the text column.\n",
    "\n",
    "    cleaned_text = ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(text) if word.isalnum() and word.lower() not in stop_words])\n",
    "    return cleaned_text\n",
    "\n",
    "# Doing the exact same but to the selftext column.\n",
    "\n",
    "df['selftext'] = df['selftext'].apply(clean_text)\n",
    "\n",
    "\n",
    "print(df['selftext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e360932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the spacy model.\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Defining a list of all the public locations that I want to understand if they are in the main dataframe.\n",
    "\n",
    "target_locations = ['Neighbourhood', 'Park', 'Recreation Area', 'Community Center', 'Library',\n",
    "                    'Places of Worship', 'School', 'tennis court', 'Cultural Center','Museum',\n",
    "                    'CoffeeShop','Cafe', 'Local Business', 'Public Transportation Hub','Local Business', 'Public Transportation Hub','Beache', 'Playground', 'Community Center','Restaurant', \n",
    "                    'Shopping Mall', 'Amusement Park', 'Stadium', 'Concert Hall', 'Theater', 'Gym', 'Fitness Center', \n",
    "                    'Public Plaza', 'Outdoor Market', 'Picnic Area', 'Recreation Center', \n",
    "                    'Conference Venue', 'Art Gallery', 'Museum', 'Garden', 'Hiking Trail', 'Sports Field', \n",
    "                    'Swimming Pool', 'Skate Park', 'Public BBQ Area', 'Historical Site', \n",
    "                    'Street Festival']\n",
    "\n",
    "# Creating a dictionary to store all the location as counts.\n",
    "\n",
    "location_counts = defaultdict(int)\n",
    "\n",
    "# Iterating through each row in the DataFrame.\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    text = row['selftext']\n",
    "    doc = nlp(text)\n",
    "\n",
    "# Tokenize all the text and then check if any of the public locations are present.\n",
    "\n",
    "    tokens = [token.text.lower() for token in doc]\n",
    "    for location in target_locations:\n",
    "        if location.lower() in tokens:\n",
    "            location_counts[location] += 1\n",
    "\n",
    "# Plot a bar chart of the most common locations and if there are no locations found then print an error message to speed up troubleshooting.\n",
    "\n",
    "top_locations = sorted(location_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "if top_locations:\n",
    "    location_labels, location_values = zip(*top_locations)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(location_labels, location_values)\n",
    "    plt.title('Most Common Locations')\n",
    "    plt.xlabel('Location')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No common locations found.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f40afb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the spacy model.\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# # Defining a list of all the public locations that I want to understand if they are in the main dataframe.\n",
    "\n",
    "target_locations = ['Neighbourhood', 'Park', 'Recreation Area', 'Community Center', 'Library',\n",
    "                    'Places of Worship', 'School', 'tennis court', 'Cultural Center', 'Museum',\n",
    "                    'CoffeeShop', 'Cafe', 'Coffee Shop', 'Local Business', 'Public Transportation Hub', 'Local Business',\n",
    "                    'Public Transportation Hub', 'Playground', 'Community Center', 'Restaurant',\n",
    "                    'Shopping Mall', 'Amusement Park', 'Stadium', 'Concert Hall', 'Theater', 'Gym', 'Fitness Center',\n",
    "                    'Public Plaza', 'Outdoor Market', 'Picnic Area', 'Recreation Center',\n",
    "                    'Conference Venue', 'Art Gallery', 'Museum', 'Garden', 'Sports Field',\n",
    "                    'Swimming Pool', 'Skate Park', 'Public BBQ Area', 'Historical Site',\n",
    "                    'Street Festival']\n",
    "\n",
    "# Creating a dictionary to store all the location as counts.\n",
    "\n",
    "location_counts = defaultdict(int)\n",
    "\n",
    "# Iterating through each row in the DataFrame.\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    text = row['selftext']\n",
    "    doc = nlp(text)\n",
    "\n",
    "# Tokenize all the text and then check if any of the public locations are present.\n",
    "\n",
    "    tokens = [token.text.lower() for token in doc]\n",
    "    for location in target_locations:\n",
    "        if location.lower() in tokens:\n",
    "            location_counts[location] += 1\n",
    "\n",
    "# Sorting the location counts and saving that to a variable.\n",
    "\n",
    "top_locations = sorted(location_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "if top_locations:\n",
    "    location_labels, location_values = zip(*top_locations)\n",
    "\n",
    "# So then each bar is a different colour I have used this code to apply custom colors for each bar.\n",
    "\n",
    "    colors = ['rgb(31, 119, 180)', 'rgb(255, 127, 14)', 'rgb(44, 160, 44)', 'rgb(214, 39, 40)',\n",
    "              'rgb(148, 103, 189)', 'rgb(140, 86, 75)', 'rgb(227, 119, 194)', 'rgb(127, 127, 127)',\n",
    "              'rgb(188, 189, 34)', 'rgb(23, 190, 207)', 'rgb(174, 199, 232)', 'rgb(255, 152, 150)',\n",
    "              'rgb(152, 223, 138)', 'rgb(255, 187, 120)', 'rgb(197, 176, 213)', 'rgb(196, 156, 148)',\n",
    "              'rgb(247, 182, 210)', 'rgb(219, 219, 141)', 'rgb(158, 218, 229)', 'rgb(23, 190, 207)',\n",
    "              'rgb(188, 189, 34)', 'rgb(23, 190, 207)', 'rgb(214, 39, 40)', 'rgb(255, 127, 14)',\n",
    "              'rgb(140, 86, 75)', 'rgb(31, 119, 180)', 'rgb(148, 103, 189)', 'rgb(227, 119, 194)',\n",
    "              'rgb(127, 127, 127)', 'rgb(188, 189, 34)', 'rgb(152, 223, 138)', 'rgb(174, 199, 232)',\n",
    "              'rgb(255, 187, 120)', 'rgb(197, 176, 213)', 'rgb(23, 190, 207)', 'rgb(196, 156, 148)',\n",
    "              'rgb(219, 219, 141)', 'rgb(247, 182, 210)', 'rgb(158, 218, 229)', 'rgb(23, 190, 207)',\n",
    "              'rgb(188, 189, 34)', 'rgb(214, 39, 40)']\n",
    "\n",
    "# Creating a bar chart using Plotly to make it easier to see and understand. \n",
    "\n",
    "    fig = go.Figure(data=[\n",
    "        go.Bar(\n",
    "            x=location_labels,\n",
    "            y=location_values,\n",
    "            marker=dict(color=colors)\n",
    "        )\n",
    "    ])\n",
    "\n",
    "# Seting the title and labelling the axis's.\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Most Common Locations',\n",
    "        xaxis_title='Location',\n",
    "        yaxis_title='Count'\n",
    "    )\n",
    "\n",
    "# Rotating the x-axis labels for better readability due to some location names being long.\n",
    "\n",
    "    fig.update_xaxes(tickangle=45)\n",
    "\n",
    "# Displaying the chart and putting a else print error message to help with troubleshooting.\n",
    "    \n",
    "    fig.show()\n",
    "else:\n",
    "    print('No common locations found.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029ef472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the VAD scores from the Warriner rescale which is loacted under the filepath.\n",
    "\n",
    "vad = pd.read_csv('/filepathname/Warriner_rescale.csv', index_col=0)\n",
    "vad = vad[['V.Mean.Sum', 'A.Mean.Sum', 'D.Mean.Sum']]\n",
    "vad.columns = ['valence', 'arousal', 'dominance']\n",
    "\n",
    "# Defining a function to get the specific VAD scores for the locations.\n",
    "\n",
    "def get_vad_scores(word):\n",
    "    if word.lower() in vad.index:\n",
    "        scores = vad.loc[word.lower()].values\n",
    "        return scores\n",
    "    else:\n",
    "        return [np.nan, np.nan, np.nan]\n",
    "    \n",
    "# Creating a new column in the dataframe to store the new VAD scores.\n",
    "\n",
    "df['vad_scores'] = [[] for _ in range(len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfad18b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defining a dictionary to store the location VAD scores.\n",
    "\n",
    "location_vad_scores = {}\n",
    "\n",
    "# Applying VAD analysis to the top four locations.\n",
    "\n",
    "for location in top_locations[:5]:\n",
    "    location_name = location[0]\n",
    "    vad_scores = get_vad_scores(location_name)\n",
    "    location_vad_scores[location_name] = vad_scores\n",
    "\n",
    "# Printing the VAD scores for the top four locations.\n",
    "\n",
    "for location, vad_scores in location_vad_scores.items():\n",
    "    print(f\"Location: {location}\")\n",
    "    print(f\"Valence: {vad_scores[0]}\")\n",
    "    print(f\"Arousal: {vad_scores[1]}\")\n",
    "    print(f\"Dominance: {vad_scores[2]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb29e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the VAD scores and names for each location.\n",
    "\n",
    "locations = list(location_vad_scores.keys())\n",
    "valence = [vad_scores[0] for vad_scores in location_vad_scores.values()]\n",
    "arousal = [vad_scores[1] for vad_scores in location_vad_scores.values()]\n",
    "dominance = [vad_scores[2] for vad_scores in location_vad_scores.values()]\n",
    "names = locations  \n",
    "\n",
    "# Creating a trace to use the scatter plot.\n",
    "\n",
    "trace = go.Scatter3d(\n",
    "    x=valence,\n",
    "    y=arousal,\n",
    "    z=dominance,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color=valence,  \n",
    "        colorscale='Viridis',  \n",
    "        opacity=0.8\n",
    "    ),\n",
    "    name='VAD Score',  # Name if the plot.\n",
    "    text=names, \n",
    "    hovertemplate='Name: %{text}<br>Valence: %{x}<br>Arousal: %{y}<br>Dominance: %{z}'  # Hover over template code for the VAD Results.\n",
    ")\n",
    "\n",
    "# Creating the layout for the plot.\n",
    "\n",
    "layout = go.Layout(\n",
    "    scene=dict(\n",
    "        xaxis=dict(title='Valence'),\n",
    "        yaxis=dict(title='Arousal'),\n",
    "        zaxis=dict(title='Dominance')\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, b=0, t=0)\n",
    ")\n",
    "\n",
    "# Creating the figure and then adding the trace and layout.\n",
    "\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "\n",
    "# Displaying the plot with the specified name.\n",
    "\n",
    "fig.update_layout(\n",
    "    title='VAD Scores for Locations'  \n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e6168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting to CSV file to save the dataframe.\n",
    "\n",
    "df.to_csv('filepathname', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
